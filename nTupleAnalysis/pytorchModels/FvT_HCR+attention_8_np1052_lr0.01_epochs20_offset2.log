2 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
2 >>  1/20 <<   Training | 0.9522 (22, 28,  7, 19) | 0.972 | 8.933 | 82.81 | 67.05 |--------------------|
2             Validation | 0.9500 (21, 28,  7, 19) | 0.971 | 4.030 | 82.88 | 67.34 |#######################| ^ (1.7%, 2.88, 0.9, 57%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1269 batches)
2 >>  2/20 <<   Training | 0.9439 (21, 30,  7, 18) | 0.965 | 3.266 | 83.67 | 68.44 |----------------------------------|
2             Validation | 0.9424 (21, 30,  7, 18) | 0.964 | 2.258 | 83.71 | 68.68 |####################################| ^ (1.3%, 2.75, 1.6, 3%) 
2 >>  3/20 <<   Training | 0.9401 (22, 28,  6, 18) | 1.007 | 10.429 | 83.89 | 69.27 |------------------------------------------|
2             Validation | 0.9388 (22, 28,  6, 18) | 1.007 | 6.143 | 83.92 | 69.46 |############################################| ^ (1.2%, 3.78, 1.5, 4%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (634 batches)
2 >>  4/20 <<   Training | 0.9376 (21, 28,  7, 19) | 0.989 | 1.304 | 83.90 | 69.14 |-----------------------------------------|
2             Validation | 0.9363 (21, 28,  7, 19) | 0.989 | 1.451 | 83.92 | 69.37 |###########################################| ^ (1.5%, 3.31, 1.5, 5%) 
2 >>  5/20 <<   Training | 0.9398 (22, 26,  7, 20) | 1.019 | 2.944 | 84.03 | 68.96 |---------------------------------------|
2             Validation | 0.9385 (22, 26,  7, 20) | 1.019 | 2.277 | 84.05 | 69.14 |#########################################| ^ (1.1%, 2.99, 1.5, 5%) 
2 >>  6/20 <<   Training | 0.9383 (20, 29,  7, 19) | 0.862 | 21.526 | 84.19 | 69.50 |--------------------------------------------|
2             Validation | 0.9374 (20, 29,  7, 19) | 0.861 | 11.399 | 84.20 | 69.59 |#############################################| ^ (0.9%, 4.17, 1.7, 1%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (317 batches)
2 >>  7/20 <<   Training | 0.9347 (22, 29,  7, 18) | 0.999 | 1.319 | 84.29 | 69.56 |---------------------------------------------|
2             Validation | 0.9338 (22, 29,  7, 18) | 0.998 | 1.488 | 84.31 | 69.73 |###############################################| ^ (1.3%, 4.02, 1.2, 19%) 
2 >>  8/20 <<   Training | 0.9359 (22, 29,  7, 18) | 1.032 | 4.615 | 84.20 | 68.75 |-------------------------------------|
2             Validation | 0.9351 (22, 29,  7, 18) | 1.032 | 2.777 | 84.22 | 68.87 |######################################| ^ (1.0%, 4.04, 1.9, 1%) 
2 >>  9/20 <<   Training | 0.9340 (21, 28,  7, 19) | 0.998 | 1.119 | 84.22 | 68.54 |-----------------------------------|
2             Validation | 0.9328 (21, 28,  7, 19) | 0.998 | 1.385 | 84.25 | 68.70 |####################################| ^ (1.3%, 3.36, 2.0, 0%) 
2 >> 10/20 <<   Training | 0.9319 (21, 29,  7, 19) | 0.972 | 1.753 | 84.34 | 68.92 |---------------------------------------|
2             Validation | 0.9313 (21, 29,  7, 19) | 0.971 | 1.791 | 84.35 | 68.99 |#######################################| ^ (1.0%, 3.54, 1.5, 6%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (158 batches)
2 >> 11/20 <<   Training | 0.9334 (23, 28,  6, 18) | 1.113 | 10.978 | 84.43 | 68.91 |---------------------------------------|
2             Validation | 0.9329 (23, 28,  6, 18) | 1.112 | 5.990 | 84.44 | 68.98 |#######################################| ^ (0.8%, 3.31, 1.6, 3%) 
2 >> 12/20 <<   Training | 0.9360 (21, 31,  6, 17) | 0.949 | 3.443 | 84.42 | 69.13 |-----------------------------------------|
2             Validation | 0.9356 (21, 31,  6, 17) | 0.948 | 2.181 | 84.43 | 69.22 |##########################################| ^ (0.8%, 3.64, 0.8, 60%) 
2 >> 13/20 <<   Training | 0.9323 (21, 30,  7, 18) | 0.943 | 4.064 | 84.44 | 69.53 |---------------------------------------------|
2             Validation | 0.9318 (21, 30,  7, 18) | 0.942 | 2.474 | 84.45 | 69.65 |##############################################| ^ (1.0%, 3.19, 1.2, 21%) 
2 >> 14/20 <<   Training | 0.9321 (20, 30,  7, 18) | 0.912 | 8.300 | 84.45 | 69.35 |-------------------------------------------|
2             Validation | 0.9319 (20, 30,  7, 18) | 0.911 | 4.719 | 84.44 | 69.39 |###########################################| ^ (0.8%, 3.23, 1.5, 5%) 
2 >> 15/20 <<   Training | 0.9324 (21, 28,  7, 20) | 0.965 | 3.118 | 84.44 | 69.09 |----------------------------------------|
2             Validation | 0.9320 (21, 28,  7, 20) | 0.964 | 2.528 | 84.45 | 69.11 |#########################################| ^ (0.8%, 3.20, 1.1, 30%) 
Decay learning rate: 0.010000 -> 0.002500
2 >> 16/20 <<   Training | 0.9302 (22, 28,  7, 19) | 1.018 | 1.260 | 84.52 | 69.34 |-------------------------------------------|
2             Validation | 0.9299 (22, 28,  7, 19) | 1.017 | 0.998 | 84.53 | 69.36 |###########################################| ^ (0.9%, 3.41, 1.0, 37%) 
Decay learning rate: 0.002500 -> 0.000625
2 >> 17/20 <<   Training | 0.9299 (22, 28,  7, 19) | 0.998 | 0.945 | 84.51 | 69.42 |--------------------------------------------|
2             Validation | 0.9296 (22, 28,  7, 19) | 0.998 | 1.280 | 84.51 | 69.45 |############################################| ^ (1.0%, 3.66, 1.4, 10%) 
Decay learning rate: 0.000625 -> 0.000156
2 >> 18/20 <<   Training | 0.9298 (22, 28,  7, 19) | 0.999 | 0.929 | 84.51 | 69.42 |--------------------------------------------|
2             Validation | 0.9295 (22, 28,  7, 19) | 0.998 | 1.162 | 84.51 | 69.45 |############################################| ^ (0.9%, 3.62, 1.2, 22%) 
Decay learning rate: 0.000156 -> 0.000039
2 >> 19/20 <<   Training | 0.9298 (22, 28,  7, 19) | 0.997 | 0.908 | 84.51 | 69.43 |--------------------------------------------|
2             Validation | 0.9295 (22, 28,  7, 19) | 0.996 | 1.142 | 84.51 | 69.46 |############################################| ^ (0.9%, 3.62, 1.2, 17%) 
Decay learning rate: 0.000039 -> 0.000010
2 >> 20/20 <<   Training | 0.9298 (22, 28,  7, 19) | 0.997 | 0.971 | 84.51 | 69.43 |--------------------------------------------|
2             Validation | 0.9295 (22, 28,  7, 19) | 0.997 | 1.129 | 84.51 | 69.46 |############################################| ^ (0.9%, 3.63, 1.2, 19%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_lr0.01_epochs20_offset2_epoch20_before_finetuning.pkl
Run Finetuning
2 >> 20/20 <<   Training | 0.9298 (22, 28,  7, 19) | 0.996 | 1.061 | 84.51 | 69.43 |--------------------------------------------|
2             Validation | 0.9295 (22, 28,  7, 19) | 0.996 | 1.058 | 84.51 | 69.46 |############################################| ^ (0.9%, 3.63, 1.3, 14%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_lr0.01_epochs20_offset2_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
